{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld5GgOmT2WE1"
      },
      "source": [
        "# Installing the Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v6kSVkV2cSS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQkjY8Jp1OpH"
      },
      "source": [
        "# Quantizing Matrices Row-Wise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 416,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/black_samorez/tensor_parallel/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaForCausalLM\n",
        "from transformers.models.llama.configuration_llama import LlamaConfig\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1: Basic Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Mapping the values to the allowed range**\n",
        "\n",
        "Quantization is the process of mapping input values from a large set to output values in a smaller set. For instance, if we consider 4-bit\n",
        "quantization, our values are represented by $4$ bits, meaning we can represent values between 0 and $2^4-1=15$.\n",
        "\n",
        " * To produce the quantized representation, we need to be able to map the matrix values to and from this range.\n",
        " * For reasons that become important later, we will perform this mapping independently for each matrix row.\n",
        " * We will parametrize the mapping like this: $out = \\frac{in}{scale} + zero$, where $scale$ and $zero$ are row-wise constants.\n",
        " * For a matrix of size `(m, k)` ($m$ rows, $k$ columns) we will aggregate those parameters into two vectors `scale` and `zero` of size `(m, 1)`.\n",
        "\n",
        "**Task 1.1:** Complete the function below to perform this mapping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "z1xDv2d12hw9"
      },
      "outputs": [],
      "source": [
        "def get_scale_and_zero(x: Tensor, max_abs: float) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\" Given a tensor x of shape (m, k) and max_abs > 0 produce tensors scale and zero of shape (m, 1) \n",
        "        such that 0 < x / scale + zero < max_abs\"\"\"\n",
        "    xmin = x.min(-1)[0]\n",
        "    xmax = x.max(-1)[0]\n",
        "\n",
        "    xmin[xmin == xmax] = -1\n",
        "    xmax[xmin == xmax] = +1\n",
        "\n",
        "    scale = (xmax - xmin) / max_abs\n",
        "    zero = -xmin / scale\n",
        "\n",
        "    scale = scale.unsqueeze(-1)\n",
        "    zero = zero.unsqueeze(-1)\n",
        "    return scale.to(x.dtype), zero.to(x.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing your code\n",
        "\n",
        "x = torch.arange(512 * 1024).reshape(512, 1024).float()\n",
        "scale, zero = get_scale_and_zero(x, 15)\n",
        "\n",
        "assert scale.shape == (512, 1), \"scale is wrong shape\"\n",
        "assert zero.shape == (512, 1), \"zero is wrong shape\"\n",
        "assert torch.all(scale * 15 <= 1023.1), \"Scale can't be that large. The resulting interval is too wide\"\n",
        "assert torch.all(scale * 15 >= 1022.9), \"Scale shouldn't be that small. The resulting interval is too narrow\"\n",
        "assert torch.all(-0.001 <  x / scale + zero) and torch.all(x / scale + zero < 15 + 0.001)\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Quantization**\n",
        "\n",
        "Having mapped the values into the allowed range, we can simply round them to obtain the quantized matrix. Complete the functions below to perform row-wise quantization. Note that:\n",
        " * `measure_and_quantize` takes the matrix `x` and `bits` - the number of bits to quantize to. Calculate the allowed quantized values range yourself (hint: look a few cells above).\n",
        " * Use `get_scale_and_zero` to obtain the layer-wise quantization constants, then use them to map the matrix values to the required range.\n",
        " * `torch.clamp(...)` the quantized values to ensure that they are in the range.\n",
        " * The function returns the quantized matrix, as well as the quantization constants, because we'll need them to dequantiza the matrix.\n",
        "\n",
        "**Task 1.2:** Complete the function below to perform quantization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quantize(x: Tensor, scale: Tensor, zero: Tensor, max_abs: float) -> Tensor:\n",
        "    \"\"\"Given a tensor x quantize it, producing tensors quantized_x\"\"\"\n",
        "    quantized_x = torch.round(x / scale + zero) \n",
        "    quantized_x = torch.clamp(quantized_x, 0, max_abs)\n",
        "    return quantized_x.to(torch.uint8)\n",
        "\n",
        "\n",
        "def dequantize(x: Tensor, scale: Tensor, zero: Tensor) -> Tensor:\n",
        "    return scale * x - scale * zero\n",
        "\n",
        "\n",
        "def measure_and_quantize(x: Tensor, bits: float) -> tuple[Tensor, Tensor, Tensor]:\n",
        "    max_abs = 2 ** bits - 1\n",
        "    scale, zero = get_scale_and_zero(x, max_abs)\n",
        "    x_quantized = quantize(x, scale, zero, max_abs)\n",
        "    return x_quantized.to(torch.uint8), scale, zero\n",
        "    \n",
        "\n",
        "# We cast the quantized matrix to uint8, but the values themselves must be in the uint<bits> range\n",
        "# This is because torch lacks support for lower bit integers\n",
        "# Obviously, we require bits <= 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Testing your code\n",
        "\n",
        "x = torch.arange(512 * 1024).reshape(512, 1024).float()\n",
        "scale, zero = get_scale_and_zero(x, 15)\n",
        "quantized_x, scale, zero = measure_and_quantize(x, 4)\n",
        "\n",
        "assert quantized_x.shape == x.shape, \"Shape of quantized_x is incorrect\"\n",
        "assert scale.shape == (512, 1), \"Shape of scale is incorrect\"\n",
        "assert zero.shape == (512, 1), \"Shape of zero is incorrect\"\n",
        "assert torch.all(quantized_x >= 0) and torch.all(quantized_x <= 15) and torch.any(quantized_x == 15), \"wrong quantized_x values range\"\n",
        "assert torch.allclose(x, dequantize(quantized_x, scale, zero), atol=50), \"Dequantized values are too far from the original values\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Using the quantized matrix**\n",
        "\n",
        "To actually use the matrix, we'll have to map it's values back into their original form. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuantizedLinear(nn.Module):\n",
        "    def __init__(self, quantized_weight, scale, zero, bias):\n",
        "        super().__init__()\n",
        "        self.quantized_weight = nn.Parameter(quantized_weight, requires_grad=False)\n",
        "        self.scale = nn.Parameter(scale, requires_grad=False)\n",
        "        self.zero = nn.Parameter(zero, requires_grad=False)\n",
        "        self.bias = nn.Parameter(bias.data.clone().detach()) if bias is not None else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, dequantize(self.quantized_weight, self.scale, self.zero), self.bias)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This class will be used as a replacement for `nn.Linear`. It holds the quantized weight and only dequantizes it during it's forward passes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2: GPTQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPTQ is the State Of The Art quantization algorithm for post-trainig DL model quantization. It works by sequentially quantizing the model's linear layer weights.\n",
        "\n",
        "Although in outputs results similar to what one would get with Round To Nearest quantization, it makes a key observations to boost it's end performance:\n",
        " * It is layer input aware, meaning int optimizes the quantized matrix to show best perfromance on inputs typically encountered in that layer.\n",
        "More formally, the problem can be formulated as:\n",
        "$$\n",
        "W_q = argmin_{\\widehat{W}}\\|XW^T - X\\widehat{W}^T\\|_2^2\n",
        "$$\n",
        ", where\n",
        " * $X$ is the input matrix of shape `(..., IN)`.\n",
        " * $XW^T$ is the unquantized output of shape `(..., OUT)`. We think of the norm above as taking a sum over those (...) dimensions.\n",
        " * $W$ is the unquantized weight of shape `(OUT, IN)`.\n",
        " * $\\widehat{W}$ is the quantized weight taken from some quantization grid.\n",
        "\n",
        "One can notice that the expression above is independent with regard to the rows of $W$ and $\\widehat{W}$, meaning we can solve it for each row in parallel. This is the reason why we're working with row-wise quantization in the first place. Notice that the quantization grid only depends on min/max values withing the row and not the quantization process, so we can think of it as fixed.\n",
        "\n",
        "and the dimension of the optimization problem is `IN`, which is too much to solve exactly. The algorithm proposes to solve it iteratively.\n",
        "\n",
        "Less us consider a vector of full precision weights $F$ and corresponding sent of inputs $X_F$. The corresponding objective is quadratic with Hessian\n",
        "$$\n",
        "H_F = 2X_FX_F^T.\n",
        "$$\n",
        "The algorithm can be described like this:\n",
        " * Do the following steps until $F$ is fully quantized:\n",
        "    1. Sample one element from $F$ randomly. Denote it by $F_i$.\n",
        "    2. Quantize the coordinate by prjecting in onto the quantization grid $Q_i = quant(F_i)$.\n",
        "    3. Update all of the remaining weights $F_: = F_: - \\frac{F_i - quant(F_i)}{\\left[H_F^{-1}\\right]_{ii}}\\cdot\\left[H_F^{-1}\\right]_{i,:}$.\n",
        "    4. Exclude $i$ from $F$.\n",
        "\n",
        "It uses the inverse Hessian to slightly tune the remaining unquantized weights to mitigate the quantization error.\n",
        "\n",
        "There are a few more ideas that make this algorithm much faster:\n",
        " 1. We can represent the random order of quantization (sampling of $i$) by permuting the row in advance, and then iterating over the row element in order.\n",
        " $$\n",
        "   F_{i:} = F_{i:} - \\frac{F_{i} - quant(F_{i})}{\\left[H_F^{-1}\\right]_{ii}}\\cdot\\left[H_F^{-1}\\right]_{i,i:}\n",
        " $$\n",
        " 2. The problem is row-wise independent, meaning that we can the same permutation each row and perform those operations in a vector fashion for all the rows at the same time.\n",
        " $$\n",
        "   F_{:,i:} = F_{:,i:} - \\frac{F_{:,i} - quant(F_{:,i})}{\\left[H_F^{-1}\\right]_{ii}}\\odot\\left[H_F^{-1}\\right]_{i,i:}\\leftarrow\\text{\\textbf{ you'll have to code this}}\n",
        " $$ \n",
        " \n",
        " 3. We dont' actually need to recompute the inverse Hessian. At $i$-th step we only need its $t$-th row, and we can use fancy math to precompute the matrix containing all of those rows in advance.\n",
        " $$\n",
        "  H^{-1} = Cholesky(H^{-1})^T    \n",
        " $$\n",
        "\n",
        " 4. We don't need to tune all the remaining unquantized values right away. We can only apply the updates for the closest elements right away and accumulate all the other updates to apply them only once in a while. \n",
        " \n",
        "    We'll do this in block of fixed size, applying the updates inside of those blocks and updating the weights outside only when we're done with the block. To accumulate those updates, we'll collect the scaled quantization error\n",
        "    $$\n",
        "      Err_{:,i} =\\frac{F_{:,i} - quant(F_{:,i})}{\\left[H_F^{-1}\\right]_{ii}}\\text{ for all }i\\text{ in block}.\n",
        "    $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**GPTQ within blocks**\n",
        "\n",
        "Implement GPTQ within the block. Iterate over the columns in ordered vector fashion, quantizing them one by one and updating all the remaining colums within the block.\n",
        "\n",
        "Return the quantized weight as well as the matrix of quantization errors that we'll need to tune the unquantized weights outside of the block.\n",
        "\n",
        "**Task 2.1:** Implement GPTQ block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 398,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gptq_block(block_weight: Tensor, block_hessian_inverse: Tensor, scale: Tensor, zero: Tensor, max_abs: float) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"NOTE: This function is allowed to alter the block_weight as we won't need those weights anymore\n",
        "\n",
        "    Args:\n",
        "        block_weight (Tensor): weight to quantize of shape (OUT, BLOCK_SIZE)\n",
        "        block_hessian_inverse (Tensor): Cholesky inverse Hessian. Upper triangular of shape (BLOCK_SIZE, BLOCK_SIZE)\n",
        "        scale (Tensor): row-wise quantization constats of shape (OUT, 1)\n",
        "        zero (Tensor): row-wise quantization constats of shape (OUT, 1)\n",
        "        max_abs (float): quantized values must lie in [0, max_abs]\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor]: quantized weight and scaled quantization error\n",
        "    \"\"\"\n",
        "    quantized_block_weight = torch.zeros(block_weight.shape, dtype=torch.uint8, device=block_weight.device)\n",
        "    scaled_block_error = torch.zeros_like(block_weight)\n",
        "    \n",
        "    # Interate over the block's columns\n",
        "    for i in range(block_weight.shape[1]):\n",
        "        # Get the column and the corresponding inverse Hessian\n",
        "        column_weight = block_weight[:, [i]]\n",
        "        column_hessian_inverse = block_hessian_inverse[i, i]\n",
        "\n",
        "        # Quantize the column weight\n",
        "        quantized_column_weight = quantize(column_weight, scale, zero, max_abs)\n",
        "        quantized_block_weight[:, [i]] = quantized_column_weight\n",
        "        dequantized_column_weight = dequantize(quantized_column_weight, scale, zero)\n",
        "\n",
        "        # Update all the following columns within the block\n",
        "        scaled_column_error = (column_weight - dequantized_column_weight) /  column_hessian_inverse\n",
        "        block_weight[:, i:] -= scaled_column_error.matmul(block_hessian_inverse[[i], i:])\n",
        "        scaled_block_error[:, [i]] = scaled_column_error\n",
        "    \n",
        "    return quantized_block_weight, scaled_block_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 399,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: how the fuck do I test it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can implement the full algorithm: \n",
        " * Get row-wise quantization constants.\n",
        " * Randomly permute the weight columns. Think about how you'd have to shuffle the Hessian as well.\n",
        " * Process the Hessian to obtain the precomputed inverse Hessian.\n",
        " * Iterate over the columns in blocks:\n",
        "    * Get the next block and quantize it.\n",
        "    * Tune all the following blocks to mitigate the quantization error.\n",
        "      $$\n",
        "         F_{:,block\\_end:} = F_{:,block\\_end:} - Err_{:,block\\_start:block\\_end}\\odot\\left[H_F^{-1}\\right]_{block\\_start:block\\_end,block\\_end:}\n",
        "      $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 400,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gptq(weight: torch.Tensor, bits: int, hessian: torch.Tensor, blocksize:int=128, percdamp:float=.01):\n",
        "    dtype = weight.dtype\n",
        "    weight = weight.clone().detach()\n",
        "    weight = weight.float()\n",
        "    num_columns = weight.shape[1]\n",
        "    \n",
        "    # Identify and patch always-zero input coordinates\n",
        "    dead = torch.diag(hessian) == 0\n",
        "    hessian[dead, dead] = 1\n",
        "    weight[:, dead] = 0\n",
        "\n",
        "    # Get row-wise quantization constants\n",
        "    maxq = torch.tensor(2 ** bits - 1)\n",
        "    scale, zero = get_scale_and_zero(weight, maxq)\n",
        "\n",
        "\n",
        "    # Randomly permute the weight columns\n",
        "    perm = torch.randperm(hessian.shape[0])\n",
        "    weight = weight[:, perm]\n",
        "    hessian = hessian[perm, :][:, perm]\n",
        "    invperm = torch.argsort(perm)\n",
        "\n",
        "    # Process the Hessian to obtain the precomputed inverse Hessian\n",
        "    damp = percdamp * torch.mean(torch.diag(hessian))\n",
        "    diag = torch.arange(num_columns, device=weight.device)\n",
        "    hessian[diag, diag] += damp\n",
        "    hessian = torch.linalg.cholesky(hessian)\n",
        "    hessian = torch.cholesky_inverse(hessian)\n",
        "    hessian = torch.linalg.cholesky(hessian, upper=True)\n",
        "    hessian_inverse = hessian\n",
        "\n",
        "    # Iterate over the columns in blocks\n",
        "    quantized_weight = torch.zeros(weight.shape, dtype=torch.uint8, device=weight.device)\n",
        "    for block_start in range(0, num_columns, blocksize):\n",
        "        block_end = min(block_start + blocksize, num_columns)\n",
        "\n",
        "        # Get the next block and quantize it\n",
        "        quantized_block_weight, block_error = gptq_block(\n",
        "            weight[:, block_start:block_end].clone(),\n",
        "            hessian_inverse[block_start:block_end, block_start:block_end],\n",
        "            scale,\n",
        "            zero,\n",
        "            maxq,\n",
        "        )\n",
        "\n",
        "        # Tune all the following blocks to mitigate the quantization error\n",
        "        quantized_weight[:, block_start:block_end] = quantized_block_weight\n",
        "        weight[:, block_end:] -= block_error.matmul(hessian_inverse[block_start:block_end, block_end:])\n",
        "\n",
        "    return quantized_weight[:, invperm], scale.to(dtype), zero.to(dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: how the fuck do I test it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPaMiUC91VYH"
      },
      "source": [
        "# LLM Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HRqro7X1eoT"
      },
      "source": [
        "## Preparations\n",
        "\n",
        "Run all the cells in this subsection to download and prepare the model and the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipghRjSA1jVN"
      },
      "source": [
        "### Download and convert the model\n",
        "\n",
        "Run the code below to download the model checkpoint and repack so that we could load the layer one by one.\n",
        " * Each layer $i \\in [0, 31]$ is saved in a separate file `\"./model/layer_{i}.bin\"`\n",
        " * Everything outside of those layers (embeddings, lm_head, etc.) is saved in `\"./model/non_layers.bin\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JIUbMToO3C9B"
      },
      "outputs": [],
      "source": [
        "!mkdir model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "87afc0a7c3504d71bc60472b02bb792f",
            "c22656a9f307432fa92c8feb5a12972c",
            "1524b51a91ae4394a74c4ccf0064e1c2",
            "7bb8cb3fc4e540d5b5bd33d51f53ef10",
            "93d32df91cf843619482784db5edf779",
            "efb6b800238e4418b7e8398ac30b5c1b",
            "cc2ea9e0c09344b3a7fe41af734a2abc",
            "e29ecbaed7ab4a67a64d7007f8ddbd3d",
            "952c67d97029484481b70d03453fcbd3",
            "3a98fcab4d6146ccb89e6db9f1463825",
            "704a09dcde84454281a712637039c40a"
          ]
        },
        "id": "j9SnAHdc19ap",
        "outputId": "79e3feb7-a5ac-4ad8-8dfb-2c671b413554"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "snapshot_download(repo_id=\"yahma/llama-7b-hf\", local_dir=\"./model\")\n",
        "\n",
        "def repack_llama(path):\n",
        "    non_layers = {}\n",
        "    first_state_dict = torch.load(os.path.join(path, \"pytorch_model-00001-of-00002.bin\"))\n",
        "    non_layers[\"model.embed_tokens.weight\"] = first_state_dict[\"model.embed_tokens.weight\"]\n",
        "    for i in trange(24):\n",
        "        layer = {key: value for key, value in first_state_dict.items() if f\"layers.{i}.\" in key}\n",
        "        torch.save(layer, os.path.join(path, f\"layer_{i}.bin\"))\n",
        "        for key in layer:\n",
        "            del first_state_dict[key]\n",
        "    del first_state_dict\n",
        "\n",
        "    second_state_dict = torch.load(os.path.join(path, \"pytorch_model-00002-of-00002.bin\"))\n",
        "    non_layers[\"lm_head.weight\"] = second_state_dict[\"lm_head.weight\"]\n",
        "    non_layers[\"model.norm.weight\"] = second_state_dict[\"model.norm.weight\"]\n",
        "    for i in trange(24, 32):\n",
        "        layer = {key: value for key, value in second_state_dict.items() if f\"layers.{i}.\" in key}\n",
        "        torch.save(layer, os.path.join(path, f\"layer_{i}.bin\"))\n",
        "        for key in layer:\n",
        "            del second_state_dict[key]\n",
        "    del second_state_dict\n",
        "\n",
        "    torch.save(non_layers, os.path.join(path, f\"non_layers.bin\"))\n",
        "\n",
        "repack_llama(\"./model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKQSag7f1nJC"
      },
      "source": [
        "### Dispatching the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To properly quantize the model we'll need two functions.\n",
        " 1. `initialize_layerless_llama` creates a llama model without any layers, but correct weights otherwise\n",
        " 2. `load_and_dispatch_a_layer` loads a layer insterts it into the model after the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "ec92f16499684b59b3bb0f8c919e2e77"
          ]
        },
        "id": "ls5meN1c4No-",
        "outputId": "ba42e966-5e51-45ef-93ec-cb60f9cdd319"
      },
      "outputs": [],
      "source": [
        "def skip(*args, **kwargs):\n",
        "    pass\n",
        "torch.nn.init.kaiming_uniform_ = skip\n",
        "torch.nn.init.uniform_ = skip\n",
        "torch.nn.init.normal_ = skip\n",
        "\n",
        "\n",
        "def initialize_layerless_llama(checkpoint_path):        \n",
        "    config = LlamaConfig.from_pretrained(\"yahma/llama-7b-hf\")\n",
        "    config.num_hidden_layers=0\n",
        "\n",
        "    model = LlamaForCausalLM(config)\n",
        "    model.load_state_dict(torch.load(os.path.join(checkpoint_path, \"non_layers.bin\")))\n",
        "    model.seqlen = 2048\n",
        "\n",
        "    return model.to(torch.float16)\n",
        "\n",
        "\n",
        "def load_and_dispatch_a_layer(layer_idx, checkpoint_path, model: LlamaForCausalLM):\n",
        "    if checkpoint_path == \"TEST\":\n",
        "        linear = nn.Linear(16, 16)\n",
        "        linear.weight.data = torch.arange(16 * 16).reshape(16, 16).float()\n",
        "        model.model.layers.append(nn.ModuleDict({\"submodule\": linear}))\n",
        "        return\n",
        "        \n",
        "    config = transformers.AutoConfig.from_pretrained(\"yahma/llama-7b-hf\")\n",
        "\n",
        "    layer = LlamaDecoderLayer(config)\n",
        "    layer_state_dict = torch.load(os.path.join(checkpoint_path, f\"layer_{layer_idx}.bin\"))\n",
        "    layer_state_dict = {name[len(f\"model.layers.{layer_idx}.\"):]: tensor for name, tensor in layer_state_dict.items()}\n",
        "    layer.load_state_dict(layer_state_dict, strict=False)\n",
        "    del layer_state_dict\n",
        "\n",
        "    model.model.layers.append(layer.to(torch.float16))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calling `initialize_layerless_llama` and then calling `load_and_dispatch_a_layer` for each layer in order would fully load the model, but we'll also quantize the layes as we go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyp_0vCs12ik"
      },
      "source": [
        "### Getting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {
        "id": "uEy1o0qZ16Kf"
      },
      "outputs": [],
      "source": [
        "def get_wikitext2(model, seed, seqlen, nsamples=128):\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
        "    trainenc = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt')\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "    return trainloader, testenc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agCFfP7x2Au6"
      },
      "source": [
        "## RTN Quantization for LLaMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Auxiliary functions:**\n",
        " * `find_layers` takes a module and returns a dictionary containing all of it's *Linear* submodules with their path-names as the keys.\n",
        " * `replace_submodule` takes a module, a path-name and a submodule and replaces the module's submodule at path-name with the new submodule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 437,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_layers(module: nn.Module, name='') -> dict[str, nn.Module]:\n",
        "    if type(module) == nn.Linear:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(find_layers(\n",
        "            child, name=name + '.' + name1 if name != '' else name1\n",
        "        ))\n",
        "    return res\n",
        "\n",
        "\n",
        "def replace_submodule(module, submodule_path, new_submodule):\n",
        "    submodule_names = submodule_path.split(\".\")\n",
        "    for submodule in submodule_names[:-1]:\n",
        "        module = getattr(module, submodule)\n",
        "    setattr(module, submodule_names[-1], new_submodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Not quantizing the model**\n",
        "\n",
        "First, take a look at the function below. It uses the functions above to load the layers one by one and iterate over their `Linear` submodules replacing them. You'll need to quantize those submodules and create the `QuantizedLinear` ones to replace the original ones with.\n",
        "\n",
        "**Task:** implement RTN quantization for LLaMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 454,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2TRwggs3Lb2F"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def llama_nearest(checkpoint_path, model, bits: int, device):\n",
        "    print('Starting ...')\n",
        "    # Load and quantize all the layers\n",
        "    layers = model.model.layers\n",
        "    for i in trange(32):\n",
        "        load_and_dispatch_a_layer(i, checkpoint_path, model)\n",
        "        layer = layers[i]\n",
        "        \n",
        "        linear_submodules = find_layers(layer)\n",
        "        for name, linear in linear_submodules.items():\n",
        "            q, scale, zero = measure_and_quantize(linear.weight.data.to(device), bits=bits)\n",
        "            quantized_linear = QuantizedLinear(q.cpu(), scale, zero, linear.bias)\n",
        "            \n",
        "            replace_submodule(layer, name, quantized_linear)\n",
        "\n",
        "        layers[i] = layer\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 455,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:00<00:00, 1848.35it/s]\n"
          ]
        }
      ],
      "source": [
        "# Testing your code\n",
        "\n",
        "model = nn.ModuleDict({\"model\": nn.ModuleDict({\"layers\": nn.ModuleList([])})})\n",
        "llama_nearest(\"TEST\", model, 4, \"cpu\")\n",
        "\n",
        "assert len(model.model.layers) == 32, \"You didn't load all the layers\"\n",
        "assert all(isinstance(layer.submodule, QuantizedLinear) for layer in model.model.layers), \"Some Linears weren't properly replaced\"\n",
        "assert torch.all(model.model.layers[0].submodule.quantized_weight == torch.arange(16).unsqueeze(0).repeat(16, 1)), \"Quantized weights are weird\"\n",
        "assert torch.all(model.model.layers[0].submodule.scale == 1), \"Quantized scales are weird\"\n",
        "assert torch.all(model.model.layers[0].submodule.scale == 1), \"Quantized scales are weird\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1SOf_pjUJXZ9"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def llama_gptq(checkpoint_path, model, dataloader, bits, device, n_samples=128):\n",
        "    print('Starting ...')\n",
        "    load_and_dispatch_a_layer(0, checkpoint_path, model)\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros(\n",
        "        (n_samples, model.seqlen, model.config.hidden_size), dtype=dtype, device=device\n",
        "    )\n",
        "    cache = {'i': 0, 'attention_mask': None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache['i']] = inp\n",
        "            cache['i'] += 1\n",
        "            cache['attention_mask'] = kwargs['attention_mask']\n",
        "            cache['position_ids'] = kwargs['position_ids']\n",
        "            raise ValueError\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for batch in dataloader:\n",
        "        try:\n",
        "            model(batch[0])\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache['attention_mask'].to(device)\n",
        "    position_ids = cache['position_ids'].to(device)\n",
        "\n",
        "    print('Ready.')\n",
        "\n",
        "    quantizers = {}\n",
        "    for i in trange(32):\n",
        "        if i != 0:\n",
        "            load_and_dispatch_a_layer(i, checkpoint_path, model)\n",
        "        layer = layers[i].to(device)\n",
        "        linear_layers = find_layers(layer)\n",
        "\n",
        "        sequential_groups = [\n",
        "            ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'],\n",
        "            ['self_attn.o_proj'],\n",
        "            ['mlp.up_proj', 'mlp.gate_proj'],\n",
        "            ['mlp.down_proj']\n",
        "        ]\n",
        "\n",
        "\n",
        "        for names in sequential_groups:\n",
        "            subset = {name: linear_layers[name] for name in names}\n",
        "\n",
        "            hessians = {name: None for name in subset}\n",
        "            num_samples = {name: 0 for name in subset}\n",
        "            def accumulate_input(name):\n",
        "                def tmp(_, inp, out):\n",
        "                    inp = inp[0].data # ... x hidden_size\n",
        "                    inp = inp.reshape((-1, inp.shape[-1])) # inputs x hidden_size\n",
        "                    inp = inp.t().float() # hidden_size x inputs\n",
        "                    num_samples[name] += 1\n",
        "                    if hessians[name] is None:\n",
        "                        hessians[name] = inp.matmul(inp.t())\n",
        "                    else:\n",
        "                        hessians[name] += inp.matmul(inp.t())\n",
        "                return tmp\n",
        "            handles = []\n",
        "            for name in subset:\n",
        "                handles.append(subset[name].register_forward_hook(accumulate_input(name)))\n",
        "            for j in range(n_samples):\n",
        "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
        "            for h in handles:\n",
        "                h.remove()\n",
        "\n",
        "            for name in subset:\n",
        "                x = subset[name].weight.data\n",
        "                bias = subset[name].bias\n",
        "                q, scale, zero = gptq(x, bits, 2 * hessians[name] / num_samples[name])\n",
        "                replace_submodule(layer, name, QuantizedLinear(q, scale, zero, bias))\n",
        "\n",
        "        for j in range(n_samples):\n",
        "            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
        "\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        inps, outs = outs, inps\n",
        "\n",
        "    model.config.use_cache = use_cache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOFpjGxX2H-J"
      },
      "source": [
        "### Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9mBdgHoVJZJ7"
      },
      "outputs": [],
      "source": [
        "class Catcher(nn.Module):\n",
        "    def __init__(self, inps_dest):\n",
        "        super().__init__()\n",
        "        self.i = 0\n",
        "        self.attention_mask = None\n",
        "        self.position_ids = None\n",
        "        self.inps_dest = inps_dest\n",
        "\n",
        "    def forward(self, inp, **kwargs):\n",
        "        self.inps_dest[self.i] = inp\n",
        "        self.attention_mask = kwargs['attention_mask']\n",
        "        self.position_ids = kwargs['position_ids']\n",
        "        self.i += 1\n",
        "        raise ValueError\n",
        "\n",
        "    def get_the_catch(self):\n",
        "        return self.attention_mask, self.position_ids\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_eval(model, testenc, device):\n",
        "    print('Evaluating ...')\n",
        "\n",
        "    input_ids = testenc.input_ids\n",
        "    input_ids = input_ids[:, :(input_ids.shape[1] // model.seqlen) *  model.seqlen]\n",
        "    input_ids = input_ids.reshape(input_ids.shape[1] // model.seqlen, model.seqlen)\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    total_nll = 0\n",
        "    for batch in torch.tensor_split(input_ids, 4):\n",
        "        n_samples = batch.shape[0]\n",
        "        dtype = next(iter(model.parameters())).dtype\n",
        "        inps = torch.zeros(\n",
        "            (n_samples, model.seqlen, model.config.hidden_size), dtype=dtype\n",
        "        ).to(device)\n",
        "        outs = torch.zeros_like(inps)\n",
        "\n",
        "        # Collect the first layer inputs\n",
        "        catcher = Catcher(inps)\n",
        "        original_layers = model.model.layers\n",
        "        model.model.layers = nn.ModuleList((catcher,))\n",
        "        for sample in batch:\n",
        "            try:\n",
        "                model(sample.unsqueeze(0))\n",
        "            except ValueError:\n",
        "                pass\n",
        "        attention_mask, position_ids = catcher.get_the_catch()\n",
        "        model.model.layers = original_layers\n",
        "\n",
        "        # Forward pass through the layers\n",
        "        layers = model.model.layers\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        position_ids = position_ids.to(device)\n",
        "        for i in trange(len(layers)):\n",
        "            layer = layers[i].to(device)\n",
        "\n",
        "            for j in range(n_samples):\n",
        "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
        "\n",
        "            layers[i] = layer.cpu()\n",
        "            del layer\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            inps, outs = outs, inps\n",
        "\n",
        "        # Calculate PPL\n",
        "        testenc = testenc.to(device)\n",
        "        nlls = []\n",
        "        for i in range(n_samples):\n",
        "            hidden_states = inps[i].unsqueeze(0)\n",
        "            if model.model.norm is not None:\n",
        "                model.model.norm = model.model.norm.to(device)\n",
        "                hidden_states = model.model.norm(hidden_states)\n",
        "                model.model.norm = model.model.norm.cpu()\n",
        "\n",
        "            model.lm_head = model.lm_head.to(device)\n",
        "            lm_logits = model.lm_head(hidden_states)\n",
        "            model.lm_head = model.lm_head.cpu()\n",
        "\n",
        "            shift_logits = lm_logits[:, :-1, :]\n",
        "            shift_labels = batch[i, 1:]\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(device))\n",
        "            total_nll += float(loss) * model.seqlen\n",
        "\n",
        "    ppl = math.exp(total_nll / input_ids.numel())\n",
        "    print(ppl)\n",
        "    model.config.use_cache = use_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCWpf5yP2N2I"
      },
      "source": [
        "### Running the whole thing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v9iFhPHlL7FB"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda:0\"\n",
        "MODEL = \"./model/\"\n",
        "SEED = 0\n",
        "BITS = 4\n",
        "N_SAMPLES = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOpM0iP4MSaa",
        "outputId": "58bbbcfe-cc29-4220-b05f-b79d1b66f7d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList()\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = initialize_layerless_llama(MODEL)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 278,
          "referenced_widgets": [
            "433f569833c34e18a7ce5eb85090f68d",
            "9ef9e6c9e7fc4b159b4ac7841e07e682",
            "5d6ba99d418d4e0a9d00b7396282aa18",
            "a788847708314c04aba0a41385c72462",
            "4305e5898b9d47b39dec021fcdb6f7b1",
            "1918541ed08c4ea1adbb025811496f4b",
            "17bac40da64a472297baab9085173d8a"
          ]
        },
        "id": "qn1bm4wlJior",
        "outputId": "5fc66958-6cfe-46ff-c378-87e84bfd2659"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "433f569833c34e18a7ce5eb85090f68d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/8.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ef9e6c9e7fc4b159b4ac7841e07e682",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/6.84k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d6ba99d418d4e0a9d00b7396282aa18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/9.62k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a788847708314c04aba0a41385c72462",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/4.72M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4305e5898b9d47b39dec021fcdb6f7b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1918541ed08c4ea1adbb025811496f4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17bac40da64a472297baab9085173d8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "dataloader, testloader = get_wikitext2(MODEL, SEED, model.seqlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zofDZ9AUZQUO",
        "outputId": "39782a91-2e32-4392-c785-efb5d242fbaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting ...\n",
            "Ready.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [44:07<00:00, 82.74s/it]\n"
          ]
        }
      ],
      "source": [
        "llama_gptq(MODEL, model, dataloader, BITS, DEVICE)\n",
        "# llama_nearest(MODEL, model, BITS, DEVICE)\n",
        "# llama_no_compression(MODEL, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TONEcUDwEoYw",
        "outputId": "fc13a143-ef33-49d8-f948-8299a3b49949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [01:38<00:00,  3.08s/it]\n",
            "100%|██████████| 32/32 [01:38<00:00,  3.08s/it]\n",
            "100%|██████████| 32/32 [01:36<00:00,  3.01s/it]\n",
            "100%|██████████| 32/32 [01:35<00:00,  2.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.934689232829368\n"
          ]
        }
      ],
      "source": [
        "llama_eval(model, testloader, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N0SbckVJoJC"
      },
      "source": [
        "FP16: 5.67\n",
        "\n",
        "GPTQx4: 5.94\n",
        "\n",
        "NEARESTx4: 6.29"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ipghRjSA1jVN",
        "agCFfP7x2Au6"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
