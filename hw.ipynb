{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing the Dependencies"
      ],
      "metadata": {
        "id": "Ld5GgOmT2WE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "2v6kSVkV2cSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization 101"
      ],
      "metadata": {
        "id": "uQkjY8Jp1OpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scale_and_zero(x, maxq):\n",
        "    tmp = torch.zeros(x.shape[:-1], device=x.device)\n",
        "    xmin = torch.minimum(x.min(-1)[0], tmp)\n",
        "    xmax = torch.maximum(x.max(-1)[0], tmp)\n",
        "\n",
        "    shape = x.shape\n",
        "    tmp = (xmin == 0) & (xmax == 0)\n",
        "    xmin[tmp] = -1\n",
        "    xmax[tmp] = +1\n",
        "\n",
        "    scale = (xmax - xmin) / maxq\n",
        "    zero = torch.round(-xmin / scale)\n",
        "\n",
        "    scale = scale.unsqueeze(-1)\n",
        "    zero = zero.unsqueeze(-1)\n",
        "    return scale.to(x.dtype), zero.to(x.dtype)\n",
        "\n",
        "\n",
        "def custom_quantize(x, bits: int):\n",
        "    x = x.clone().detach()\n",
        "    maxq = torch.tensor(2 ** bits - 1)\n",
        "    scale, zero = get_scale_and_zero(x, maxq)\n",
        "\n",
        "    q = torch.clamp(torch.round(x / scale) + zero, 0, maxq)\n",
        "    return q.to(torch.uint8), scale, zero\n",
        "\n",
        "\n",
        "class QuantizedLinear(nn.Module):\n",
        "    def __init__(self, q, scale, zero, bias):\n",
        "        super().__init__()\n",
        "        self.q = nn.Parameter(q, requires_grad=False)\n",
        "        self.scale = nn.Parameter(scale, requires_grad=False)\n",
        "        self.zero = nn.Parameter(zero, requires_grad=False)\n",
        "\n",
        "        if bias is not None:\n",
        "            self.bias = nn.Parameter(bias.data.clone().detach())\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self.scale * self.q - self.scale * self.zero, self.bias)"
      ],
      "metadata": {
        "id": "z1xDv2d12hw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Quantization"
      ],
      "metadata": {
        "id": "UPaMiUC91VYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparations"
      ],
      "metadata": {
        "id": "2HRqro7X1eoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and convert the model"
      ],
      "metadata": {
        "id": "ipghRjSA1jVN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JIUbMToO3C9B"
      },
      "outputs": [],
      "source": [
        "!mkdir model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "87afc0a7c3504d71bc60472b02bb792f",
            "c22656a9f307432fa92c8feb5a12972c",
            "1524b51a91ae4394a74c4ccf0064e1c2",
            "7bb8cb3fc4e540d5b5bd33d51f53ef10",
            "93d32df91cf843619482784db5edf779",
            "efb6b800238e4418b7e8398ac30b5c1b",
            "cc2ea9e0c09344b3a7fe41af734a2abc",
            "e29ecbaed7ab4a67a64d7007f8ddbd3d",
            "952c67d97029484481b70d03453fcbd3",
            "3a98fcab4d6146ccb89e6db9f1463825",
            "704a09dcde84454281a712637039c40a"
          ]
        },
        "id": "j9SnAHdc19ap",
        "outputId": "79e3feb7-a5ac-4ad8-8dfb-2c671b413554"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87afc0a7c3504d71bc60472b02bb792f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c22656a9f307432fa92c8feb5a12972c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)ca2ada75/config.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1524b51a91ae4394a74c4ccf0064e1c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)ada75/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bb8cb3fc4e540d5b5bd33d51f53ef10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93d32df91cf843619482784db5edf779",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efb6b800238e4418b7e8398ac30b5c1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc2ea9e0c09344b3a7fe41af734a2abc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)27ca2ada75/README.md:   0%|          | 0.00/8.84k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e29ecbaed7ab4a67a64d7007f8ddbd3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/207 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "952c67d97029484481b70d03453fcbd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a98fcab4d6146ccb89e6db9f1463825",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "704a09dcde84454281a712637039c40a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [00:42<00:00,  1.75s/it]\n",
            "100%|██████████| 8/8 [00:16<00:00,  2.03s/it]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm, trange\n",
        "import json\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "snapshot_download(repo_id=\"yahma/llama-7b-hf\", local_dir=\"./model\")\n",
        "\n",
        "def repack_llama(path):\n",
        "    non_layers = {}\n",
        "    first_state_dict = torch.load(os.path.join(path, \"pytorch_model-00001-of-00002.bin\"))\n",
        "    non_layers[\"model.embed_tokens.weight\"] = first_state_dict[\"model.embed_tokens.weight\"]\n",
        "    for i in trange(24):\n",
        "        layer = {key: value for key, value in first_state_dict.items() if f\"layers.{i}.\" in key}\n",
        "        torch.save(layer, os.path.join(path, f\"layer_{i}.bin\"))\n",
        "        for key in layer:\n",
        "            del first_state_dict[key]\n",
        "    del first_state_dict\n",
        "\n",
        "    second_state_dict = torch.load(os.path.join(path, \"pytorch_model-00002-of-00002.bin\"))\n",
        "    non_layers[\"lm_head.weight\"] = second_state_dict[\"lm_head.weight\"]\n",
        "    non_layers[\"model.norm.weight\"] = second_state_dict[\"model.norm.weight\"]\n",
        "    for i in trange(24, 32):\n",
        "        layer = {key: value for key, value in second_state_dict.items() if f\"layers.{i}.\" in key}\n",
        "        torch.save(layer, os.path.join(path, f\"layer_{i}.bin\"))\n",
        "        for key in layer:\n",
        "            del second_state_dict[key]\n",
        "    del second_state_dict\n",
        "\n",
        "    torch.save(non_layers, os.path.join(path, f\"non_layers.bin\"))\n",
        "\n",
        "repack_llama(\"./model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dispatching the model"
      ],
      "metadata": {
        "id": "MKQSag7f1nJC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "ec92f16499684b59b3bb0f8c919e2e77"
          ]
        },
        "id": "ls5meN1c4No-",
        "outputId": "ba42e966-5e51-45ef-93ec-cb60f9cdd319"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec92f16499684b59b3bb0f8c919e2e77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "torch.set_num_threads(8)\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaForCausalLM\n",
        "from transformers.models.llama.configuration_llama import LlamaConfig\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def skip(*args, **kwargs):\n",
        "    pass\n",
        "torch.nn.init.kaiming_uniform_ = skip\n",
        "torch.nn.init.uniform_ = skip\n",
        "torch.nn.init.normal_ = skip\n",
        "\n",
        "\n",
        "def initialize_layerless_llama(checkpoint_path):\n",
        "    config = LlamaConfig.from_pretrained(\"yahma/llama-7b-hf\")\n",
        "    config.num_hidden_layers=0\n",
        "\n",
        "    model = LlamaForCausalLM(config)\n",
        "    model.load_state_dict(torch.load(os.path.join(checkpoint_path, \"non_layers.bin\")))\n",
        "    model.seqlen = 2048\n",
        "\n",
        "    return model.to(torch.float16)\n",
        "\n",
        "\n",
        "def load_and_dispatch_a_layer(layer_idx, checkpoint_path, model: LlamaForCausalLM):\n",
        "    config = transformers.AutoConfig.from_pretrained(\"yahma/llama-7b-hf\")\n",
        "\n",
        "    layer = LlamaDecoderLayer(config)\n",
        "    layer_state_dict = torch.load(os.path.join(checkpoint_path, f\"layer_{layer_idx}.bin\"))\n",
        "    layer_state_dict = {name[len(f\"model.layers.{layer_idx}.\"):]: tensor for name, tensor in layer_state_dict.items()}\n",
        "    layer.load_state_dict(layer_state_dict, strict=False)\n",
        "    del layer_state_dict\n",
        "\n",
        "    model.model.layers.append(layer.to(torch.float16))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the data"
      ],
      "metadata": {
        "id": "iyp_0vCs12ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikitext2(model, seed, seqlen, nsamples=128):\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
        "    trainenc = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt')\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "    return trainloader, testenc"
      ],
      "metadata": {
        "id": "uEy1o0qZ16Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantizing"
      ],
      "metadata": {
        "id": "Q68GAqtf172N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lJ9tBgf4JTjG"
      },
      "outputs": [],
      "source": [
        "def gptq(weight: torch.Tensor, bits: int, hessian: torch.Tensor, blocksize:int=128, percdamp:float=.01):\n",
        "    dtype = weight.dtype\n",
        "    weight = weight.clone().detach()\n",
        "    weight = weight.float()\n",
        "    num_columns = weight.shape[1]\n",
        "\n",
        "    # Get per-row quantization boundaries\n",
        "    maxq = torch.tensor(2 ** bits - 1)\n",
        "    scale, zero = get_scale_and_zero(weight, maxq)\n",
        "\n",
        "    # Identify and patch always-zero inputs\n",
        "    dead = torch.diag(hessian) == 0\n",
        "    hessian[dead, dead] = 1\n",
        "    weight[:, dead] = 0\n",
        "\n",
        "    # Decrasing activation size\n",
        "    perm = torch.argsort(torch.diag(hessian), descending=True)\n",
        "    weight = weight[:, perm]\n",
        "    hessian = hessian[perm, :][:, perm]\n",
        "    invperm = torch.argsort(perm)\n",
        "\n",
        "    # Process the Hessian\n",
        "    damp = percdamp * torch.mean(torch.diag(hessian))\n",
        "    diag = torch.arange(num_columns, device=weight.device)\n",
        "    hessian[diag, diag] += damp\n",
        "    hessian = torch.linalg.cholesky(hessian)\n",
        "    hessian = torch.cholesky_inverse(hessian)\n",
        "    hessian = torch.linalg.cholesky(hessian, upper=True)\n",
        "    hessian_inverse = hessian\n",
        "\n",
        "    quantized_weight = torch.zeros(weight.shape, dtype=torch.uint8, device=weight.device)\n",
        "    # Iterate over columns in blocks\n",
        "    for left in range(0, num_columns, blocksize):\n",
        "        right = min(left + blocksize, num_columns)\n",
        "        count = right - left\n",
        "\n",
        "        # Get the next block\n",
        "        block_weight = weight[:, left:right].clone()\n",
        "        quantized_block_weight = torch.zeros(block_weight.shape, dtype=torch.uint8, device=block_weight.device)\n",
        "        block_error = torch.zeros_like(block_weight)\n",
        "        block_hessian_inverse = hessian_inverse[left:right, left:right]\n",
        "\n",
        "        # Interate over the block's columns\n",
        "        for i in range(count):\n",
        "            # Get the column and the corresponding inverse Hessian\n",
        "            column_weight = block_weight[:, i]\n",
        "            column_hessian_inverse = block_hessian_inverse[i, i]\n",
        "\n",
        "            # Quantize the column weight\n",
        "            quantized_column_weight = torch.clamp(torch.round(column_weight.unsqueeze(1) / scale) + zero, 0, maxq).flatten()\n",
        "            quantized_block_weight[:, i] = quantized_column_weight.to(torch.uint8)\n",
        "            dequantized_column_weight = scale.flatten() * quantized_column_weight - scale.flatten() * zero.flatten()\n",
        "\n",
        "            # Update all the following columns within the block\n",
        "            column_error = (column_weight - dequantized_column_weight) / column_hessian_inverse\n",
        "            block_weight[:, i:] -= column_error.unsqueeze(1).matmul(block_hessian_inverse[i, i:].unsqueeze(0))\n",
        "            block_error[:, i] = column_error\n",
        "\n",
        "        # Update all the following blocks\n",
        "        quantized_weight[:, left:right] = quantized_block_weight\n",
        "        weight[:, right:] -= block_error.matmul(hessian_inverse[left:right, right:])\n",
        "\n",
        "    return quantized_weight[:, invperm], scale.to(dtype), zero.to(dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer-wise quantizations"
      ],
      "metadata": {
        "id": "agCFfP7x2Au6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2TRwggs3Lb2F"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def llama_no_compression(checkpoint_path, model):\n",
        "    print('Starting ...')\n",
        "    # Load all the layers\n",
        "    layers = model.model.layers\n",
        "    for i in trange(32):\n",
        "        load_and_dispatch_a_layer(i, checkpoint_path, model)\n",
        "\n",
        "\n",
        "def find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):\n",
        "    if type(module) in layers:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(find_layers(\n",
        "            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n",
        "        ))\n",
        "    return res\n",
        "\n",
        "\n",
        "def replace_submodule(module, submodule_path, new_submodule):\n",
        "    submodule_names = submodule_path.split(\".\")\n",
        "    for submodule in submodule_names[:-1]:\n",
        "        module = getattr(module, submodule)\n",
        "    setattr(module, submodule_names[-1], new_submodule)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_nearest(checkpoint_path, model, bits: int, device):\n",
        "    print('Starting ...')\n",
        "\n",
        "    # Load and quantize all the layers\n",
        "    layers = model.model.layers\n",
        "    for i in trange(32):\n",
        "        load_and_dispatch_a_layer(i, checkpoint_path, model)\n",
        "\n",
        "        layer = layers[i]\n",
        "        linear_submodules = find_layers(layer)\n",
        "\n",
        "        sequential_groups = [\n",
        "            ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'],\n",
        "            ['self_attn.o_proj'],\n",
        "            ['mlp.up_proj', 'mlp.gate_proj'],\n",
        "            ['mlp.down_proj']\n",
        "        ]\n",
        "\n",
        "        for group_names in sequential_groups:\n",
        "            current_linears_to_quantize = {n: linear_submodules[n] for n in group_names}\n",
        "            for name, linear in current_linears_to_quantize.items():\n",
        "                q, scale, zero = custom_quantize(linear.weight.data.to(device), bits=bits)\n",
        "                replace_submodule(layer, name, QuantizedLinear(q.cpu(), scale, zero, linear.bias))\n",
        "\n",
        "        layers[i] = layer\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1SOf_pjUJXZ9"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def llama_gptq(checkpoint_path, model, dataloader, bits, device, n_samples=128):\n",
        "    print('Starting ...')\n",
        "    load_and_dispatch_a_layer(0, checkpoint_path, model)\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros(\n",
        "        (n_samples, model.seqlen, model.config.hidden_size), dtype=dtype, device=device\n",
        "    )\n",
        "    cache = {'i': 0, 'attention_mask': None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache['i']] = inp\n",
        "            cache['i'] += 1\n",
        "            cache['attention_mask'] = kwargs['attention_mask']\n",
        "            cache['position_ids'] = kwargs['position_ids']\n",
        "            raise ValueError\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for batch in dataloader:\n",
        "        try:\n",
        "            model(batch[0])\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache['attention_mask'].to(device)\n",
        "    position_ids = cache['position_ids'].to(device)\n",
        "\n",
        "    print('Ready.')\n",
        "\n",
        "    quantizers = {}\n",
        "    for i in trange(32):\n",
        "        if i != 0:\n",
        "            load_and_dispatch_a_layer(i, checkpoint_path, model)\n",
        "        layer = layers[i].to(device)\n",
        "        linear_layers = find_layers(layer)\n",
        "\n",
        "        sequential_groups = [\n",
        "            ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'],\n",
        "            ['self_attn.o_proj'],\n",
        "            ['mlp.up_proj', 'mlp.gate_proj'],\n",
        "            ['mlp.down_proj']\n",
        "        ]\n",
        "\n",
        "\n",
        "        for names in sequential_groups:\n",
        "            subset = {name: linear_layers[name] for name in names}\n",
        "\n",
        "            hessians = {name: None for name in subset}\n",
        "            num_samples = {name: 0 for name in subset}\n",
        "            def accumulate_input(name):\n",
        "                def tmp(_, inp, out):\n",
        "                    inp = inp[0].data # ... x hidden_size\n",
        "                    inp = inp.reshape((-1, inp.shape[-1])) # inputs x hidden_size\n",
        "                    inp = inp.t().float() # hidden_size x inputs\n",
        "                    num_samples[name] += 1\n",
        "                    if hessians[name] is None:\n",
        "                        hessians[name] = inp.matmul(inp.t())\n",
        "                    else:\n",
        "                        hessians[name] += inp.matmul(inp.t())\n",
        "                return tmp\n",
        "            handles = []\n",
        "            for name in subset:\n",
        "                handles.append(subset[name].register_forward_hook(accumulate_input(name)))\n",
        "            for j in range(n_samples):\n",
        "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
        "            for h in handles:\n",
        "                h.remove()\n",
        "\n",
        "            for name in subset:\n",
        "                x = subset[name].weight.data\n",
        "                bias = subset[name].bias\n",
        "                q, scale, zero = gptq(x, bits, 2 * hessians[name] / num_samples[name])\n",
        "                replace_submodule(layer, name, QuantizedLinear(q, scale, zero, bias))\n",
        "\n",
        "        for j in range(n_samples):\n",
        "            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
        "\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        inps, outs = outs, inps\n",
        "\n",
        "    model.config.use_cache = use_cache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating"
      ],
      "metadata": {
        "id": "EOFpjGxX2H-J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9mBdgHoVJZJ7"
      },
      "outputs": [],
      "source": [
        "class Catcher(nn.Module):\n",
        "    def __init__(self, inps_dest):\n",
        "        super().__init__()\n",
        "        self.i = 0\n",
        "        self.attention_mask = None\n",
        "        self.position_ids = None\n",
        "        self.inps_dest = inps_dest\n",
        "\n",
        "    def forward(self, inp, **kwargs):\n",
        "        self.inps_dest[self.i] = inp\n",
        "        self.attention_mask = kwargs['attention_mask']\n",
        "        self.position_ids = kwargs['position_ids']\n",
        "        self.i += 1\n",
        "        raise ValueError\n",
        "\n",
        "    def get_the_catch(self):\n",
        "        return self.attention_mask, self.position_ids\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_eval(model, testenc, device):\n",
        "    print('Evaluating ...')\n",
        "\n",
        "    input_ids = testenc.input_ids\n",
        "    input_ids = input_ids[:, :(input_ids.shape[1] // model.seqlen) *  model.seqlen]\n",
        "    input_ids = input_ids.reshape(input_ids.shape[1] // model.seqlen, model.seqlen)\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    total_nll = 0\n",
        "    for batch in torch.tensor_split(input_ids, 4):\n",
        "        n_samples = batch.shape[0]\n",
        "        dtype = next(iter(model.parameters())).dtype\n",
        "        inps = torch.zeros(\n",
        "            (n_samples, model.seqlen, model.config.hidden_size), dtype=dtype\n",
        "        ).to(device)\n",
        "        outs = torch.zeros_like(inps)\n",
        "\n",
        "        # Collect the first layer inputs\n",
        "        catcher = Catcher(inps)\n",
        "        original_layers = model.model.layers\n",
        "        model.model.layers = nn.ModuleList((catcher,))\n",
        "        for sample in batch:\n",
        "            try:\n",
        "                model(sample.unsqueeze(0))\n",
        "            except ValueError:\n",
        "                pass\n",
        "        attention_mask, position_ids = catcher.get_the_catch()\n",
        "        model.model.layers = original_layers\n",
        "\n",
        "        # Forward pass through the layers\n",
        "        layers = model.model.layers\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        position_ids = position_ids.to(device)\n",
        "        for i in trange(len(layers)):\n",
        "            layer = layers[i].to(device)\n",
        "\n",
        "            for j in range(n_samples):\n",
        "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
        "\n",
        "            layers[i] = layer.cpu()\n",
        "            del layer\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            inps, outs = outs, inps\n",
        "\n",
        "        # Calculate PPL\n",
        "        testenc = testenc.to(device)\n",
        "        nlls = []\n",
        "        for i in range(n_samples):\n",
        "            hidden_states = inps[i].unsqueeze(0)\n",
        "            if model.model.norm is not None:\n",
        "                model.model.norm = model.model.norm.to(device)\n",
        "                hidden_states = model.model.norm(hidden_states)\n",
        "                model.model.norm = model.model.norm.cpu()\n",
        "\n",
        "            model.lm_head = model.lm_head.to(device)\n",
        "            lm_logits = model.lm_head(hidden_states)\n",
        "            model.lm_head = model.lm_head.cpu()\n",
        "\n",
        "            shift_logits = lm_logits[:, :-1, :]\n",
        "            shift_labels = batch[i, 1:]\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(device))\n",
        "            total_nll += float(loss) * model.seqlen\n",
        "\n",
        "    ppl = math.exp(total_nll / input_ids.numel())\n",
        "    print(ppl)\n",
        "    model.config.use_cache = use_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the whole thing"
      ],
      "metadata": {
        "id": "DCWpf5yP2N2I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v9iFhPHlL7FB"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda:0\"\n",
        "MODEL = \"./model/\"\n",
        "SEED = 0\n",
        "BITS = 4\n",
        "N_SAMPLES = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOpM0iP4MSaa",
        "outputId": "58bbbcfe-cc29-4220-b05f-b79d1b66f7d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList()\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = initialize_layerless_llama(MODEL)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 278,
          "referenced_widgets": [
            "433f569833c34e18a7ce5eb85090f68d",
            "9ef9e6c9e7fc4b159b4ac7841e07e682",
            "5d6ba99d418d4e0a9d00b7396282aa18",
            "a788847708314c04aba0a41385c72462",
            "4305e5898b9d47b39dec021fcdb6f7b1",
            "1918541ed08c4ea1adbb025811496f4b",
            "17bac40da64a472297baab9085173d8a"
          ]
        },
        "id": "qn1bm4wlJior",
        "outputId": "5fc66958-6cfe-46ff-c378-87e84bfd2659"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "433f569833c34e18a7ce5eb85090f68d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/8.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ef9e6c9e7fc4b159b4ac7841e07e682",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/6.84k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d6ba99d418d4e0a9d00b7396282aa18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/9.62k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a788847708314c04aba0a41385c72462",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/4.72M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4305e5898b9d47b39dec021fcdb6f7b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1918541ed08c4ea1adbb025811496f4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17bac40da64a472297baab9085173d8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "dataloader, testloader = get_wikitext2(MODEL, SEED, model.seqlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zofDZ9AUZQUO",
        "outputId": "39782a91-2e32-4392-c785-efb5d242fbaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting ...\n",
            "Ready.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [44:07<00:00, 82.74s/it]\n"
          ]
        }
      ],
      "source": [
        "llama_gptq(MODEL, model, dataloader, BITS, DEVICE)\n",
        "# llama_nearest(MODEL, model, BITS, DEVICE)\n",
        "# llama_no_compression(MODEL, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TONEcUDwEoYw",
        "outputId": "fc13a143-ef33-49d8-f948-8299a3b49949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [01:38<00:00,  3.08s/it]\n",
            "100%|██████████| 32/32 [01:38<00:00,  3.08s/it]\n",
            "100%|██████████| 32/32 [01:36<00:00,  3.01s/it]\n",
            "100%|██████████| 32/32 [01:35<00:00,  2.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.934689232829368\n"
          ]
        }
      ],
      "source": [
        "llama_eval(model, testloader, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N0SbckVJoJC"
      },
      "source": [
        "FP16: 5.67\n",
        "\n",
        "GPTQx4: 5.94\n",
        "\n",
        "NEARESTx4: 6.29"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZpQylPmwVRS"
      },
      "source": [
        "## BONUS: QUIK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maSZXogTJmZk"
      },
      "outputs": [],
      "source": [
        "def quik(x: torch.Tensor, bits: int, hessian: torch.Tensor, blocksize:int=128, percdamp:float=.01, n_outliers=128):\n",
        "    dtype = x.dtype\n",
        "    W = x.clone().detach()\n",
        "    W = W.float()\n",
        "\n",
        "    H = hessian\n",
        "    dead = torch.diag(H) == 0\n",
        "    H[dead, dead] = 1\n",
        "    W[:, dead] = 0\n",
        "    # decrasing activation size\n",
        "    perm = torch.argsort(torch.diag(H), descending=True)\n",
        "    W = W[:, perm]\n",
        "    H = H[perm, :][:, perm]\n",
        "\n",
        "    # Process outliers\n",
        "    outlier_weight = W[:,:n_outliers]\n",
        "    W = W[:,n_outliers:]\n",
        "    columns = W.shape[1]\n",
        "    H = H[n_outliers:,:][:,n_outliers:]\n",
        "\n",
        "    maxq = torch.tensor(2 ** (bits - 1) - 1).to(W.device)\n",
        "    scale, zero = get_scale_and_zero(W, maxq)\n",
        "\n",
        "    Losses = torch.zeros_like(W)\n",
        "    Q = torch.zeros(W.shape, dtype=torch.int8, device=W.device)\n",
        "\n",
        "    damp = percdamp * torch.mean(torch.diag(H))\n",
        "    diag = torch.arange(columns, device=W.device)\n",
        "    H[diag, diag] += damp\n",
        "    H = torch.linalg.cholesky(H)\n",
        "    H = torch.cholesky_inverse(H)\n",
        "    H = torch.linalg.cholesky(H, upper=True)\n",
        "    Hinv = H\n",
        "\n",
        "    for i1 in range(0, columns, blocksize):\n",
        "        i2 = min(i1 + blocksize, columns)\n",
        "        count = i2 - i1\n",
        "\n",
        "        W1 = W[:, i1:i2].clone()\n",
        "        Q1 = torch.zeros(W1.shape, dtype=torch.int8, device=W1.device)\n",
        "        Err1 = torch.zeros_like(W1)\n",
        "        Losses1 = torch.zeros_like(W1)\n",
        "        Hinv1 = Hinv[i1:i2, i1:i2]\n",
        "\n",
        "        for i in range(count):\n",
        "            w = W1[:, i]\n",
        "            d = Hinv1[i, i]\n",
        "\n",
        "            q = torch.clamp(torch.round((w.unsqueeze(1) - zero) / scale), -maxq, maxq).flatten()\n",
        "            Q1[:, i] = q.to(torch.uint8)\n",
        "            q = scale.flatten() * q - scale.flatten() * zero.flatten()\n",
        "\n",
        "            Losses1[:, i] = (w - q) ** 2 / d ** 2\n",
        "\n",
        "            err1 = (w - q) / d\n",
        "            W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n",
        "            Err1[:, i] = err1\n",
        "\n",
        "        Q[:, i1:i2] = Q1\n",
        "        Losses[:, i1:i2] = Losses1 / 2\n",
        "\n",
        "        W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n",
        "    return Q, scale.to(dtype), zero.to(dtype), outlier_weight.to(dtype), perm\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_quik(checkpoint_path, model, dataloader, bits, device, n_samples=128, n_outliers=128):\n",
        "    print('Starting ...')\n",
        "    load_and_dispatch_a_layer(0, checkpoint_path, model)\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros(\n",
        "        (n_samples, model.seqlen, model.config.hidden_size), dtype=dtype, device=device\n",
        "    )\n",
        "    cache = {'i': 0, 'attention_mask': None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache['i']] = inp\n",
        "            cache['i'] += 1\n",
        "            cache['attention_mask'] = kwargs['attention_mask']\n",
        "            cache['position_ids'] = kwargs['position_ids']\n",
        "            raise ValueError\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for batch in dataloader:\n",
        "        try:\n",
        "            model(batch[0])\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache['attention_mask'].to(device)\n",
        "    position_ids = cache['position_ids'].to(device)\n",
        "\n",
        "    print('Ready.')\n",
        "\n",
        "    quantizers = {}\n",
        "    for i in trange(1):\n",
        "        if i != 0:\n",
        "            load_and_dispatch_a_layer(i, checkpoint_path, model)\n",
        "        layer = layers[i].to(device)\n",
        "        linear_layers = find_layers(layer)\n",
        "\n",
        "        sequential_groups = [\n",
        "            ['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'],\n",
        "            ['self_attn.o_proj'],\n",
        "            ['mlp.up_proj', 'mlp.gate_proj'],\n",
        "            ['mlp.down_proj']\n",
        "        ]\n",
        "\n",
        "\n",
        "        for names in sequential_groups:\n",
        "            subset = {name: linear_layers[name] for name in names}\n",
        "\n",
        "            hessians = {name: None for name in subset}\n",
        "            num_samples = {name: 0 for name in subset}\n",
        "            def accumulate_input(name):\n",
        "                def tmp(_, inp, out):\n",
        "                    inp = inp[0].data # ... x hidden_size\n",
        "                    inp = inp.reshape((-1, inp.shape[-1])) # inputs x hidden_size\n",
        "                    inp = inp.t().float() # hidden_size x inputs\n",
        "                    num_samples[name] += 1\n",
        "                    if hessians[name] is None:\n",
        "                        hessians[name] = inp.matmul(inp.t())\n",
        "                    else:\n",
        "                        hessians[name] += inp.matmul(inp.t())\n",
        "                return tmp\n",
        "            handles = []\n",
        "            for name in subset:\n",
        "                handles.append(subset[name].register_forward_hook(accumulate_input(name)))\n",
        "            for j in range(n_samples):\n",
        "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
        "            if \"self_attn.o_proj\" in names:\n",
        "                return\n",
        "            for h in handles:\n",
        "                h.remove()\n",
        "\n",
        "            for name in subset:\n",
        "                print(name)\n",
        "                x = subset[name].weight.data\n",
        "                bias = subset[name].bias\n",
        "                q, scale, zero, outlier_weight, perm = quik(x, bits, 2 * hessians[name] / num_samples[name], n_outliers=n_outliers)\n",
        "                replace_submodule(layer, name, QuikLinear(q, scale, zero, outlier_weight, bias, bits, perm))\n",
        "\n",
        "        for j in range(n_samples):\n",
        "            outs[j] = layer(inps[j].unsqueeze(0).to(device), attention_mask=attention_mask, position_ids=position_ids)[0].cpu()\n",
        "\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        inps, outs = outs, inps\n",
        "\n",
        "    model.config.use_cache = use_cache\n",
        "\n",
        "\n",
        "class QuikLinear(nn.Module):\n",
        "    def __init__(self, quantized_weight, weight_scale, weight_zero, outlier_weight, bias, bits: int, perm):\n",
        "        super().__init__()\n",
        "        self.bits = bits\n",
        "        self.perm = perm\n",
        "        self.n_outliers = outlier_weight.shape[1]\n",
        "\n",
        "        self.quantized_weight = nn.Parameter(quantized_weight, requires_grad=False)\n",
        "        self.weight_scale = nn.Parameter(weight_scale, requires_grad=False)\n",
        "        self.weight_zero = nn.Parameter(weight_zero, requires_grad=False)\n",
        "\n",
        "        self.outlier_weight = nn.Parameter(outlier_weight, requires_grad=False)\n",
        "        self.weights_reduced = (self.quantized_weight * self.weight_scale).mean(axis=1)\n",
        "\n",
        "        if bias is not None:\n",
        "            self.bias = nn.Parameter(bias.data.clone().detach())\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input[...,self.perm]\n",
        "        input_quantized, input_scale, input_zero = custom_quantize(input[...,self.n_outliers:], self.bits)\n",
        "        inputs_reduced = (input_quantized * input_scale).sum(axis=-1)\n",
        "\n",
        "        out_size, in_size = self.quantized_weight.shape\n",
        "\n",
        "        quantized_result = F.linear(\n",
        "            input_quantized       * input_scale,\n",
        "            self.quantized_weight * self.weight_scale,\n",
        "        )\n",
        "\n",
        "        quantized_result += (input_zero @ self.weight_zero.T) * out_size\n",
        "        quantized_result += (input_zero @ self.weights_reduced.unsqueeze(0)) * out_size\n",
        "        quantized_result += inputs_reduced.unsqueeze(-1) @ self.weight_zero.T\n",
        "\n",
        "        outliers_result = F.linear(input[...,:self.n_outliers], self.outlier_weight, self.bias)\n",
        "\n",
        "        results = quantized_result + outliers_result\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRmwpzVEz2wX"
      },
      "outputs": [],
      "source": [
        "# model = initialize_layerless_llama(MODEL)\n",
        "# model.eval()\n",
        "# llama_quik(MODEL, model, dataloader, BITS, DEVICE, n_outliers=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiAy9yoPR958"
      },
      "outputs": [],
      "source": [
        "# model = initialize_layerless_llama(MODEL)\n",
        "# model.eval()\n",
        "# llama_gptq(MODEL, model, dataloader, BITS, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A865GXAT4mwo"
      },
      "source": [
        "## LAST FULL RUN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNI7I3c9z4x0",
        "outputId": "a86c749b-b1af-45af-85ee-8f52979a23da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [01:57<00:00,  3.68s/it]\n",
            "100%|██████████| 32/32 [01:58<00:00,  3.69s/it]\n",
            "100%|██████████| 32/32 [01:55<00:00,  3.62s/it]\n",
            "100%|██████████| 32/32 [01:55<00:00,  3.60s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nan\n"
          ]
        }
      ],
      "source": [
        "llama_eval(model, testloader, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rhjQrDm2I8p",
        "outputId": "760474db-3bb7-4958-ec1c-2a2fa3fcb5ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[3, 0, 0,  ..., 0, 0, 0],\n",
              "        [6, 0, 0,  ..., 0, 0, 0],\n",
              "        [6, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [7, 0, 0,  ..., 0, 0, 0],\n",
              "        [9, 0, 0,  ..., 0, 0, 0],\n",
              "        [6, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.model.layers[0].self_attn.o_proj.quantized_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SegLcoJoA6PH"
      },
      "source": [
        "## TEMP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aokiGXmFxCTy"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS9Uyt8UaBdO"
      },
      "outputs": [],
      "source": [
        "model = initialize_layerless_llama(MODEL)\n",
        "inps = torch.zeros(\n",
        "    (N_SAMPLES, model.seqlen, model.config.hidden_size), dtype=torch.float16\n",
        ").to(DEVICE)\n",
        "\n",
        "# Collect the first layer inputs\n",
        "catcher = Catcher(inps)\n",
        "model.model.layers = nn.ModuleList((catcher,))\n",
        "for sample in dataloader:\n",
        "    try:\n",
        "        model(sample[0])\n",
        "    except ValueError:\n",
        "        pass\n",
        "attention_mask, position_ids = catcher.get_the_catch()\n",
        "model.model.layers = nn.ModuleList()\n",
        "load_and_dispatch_a_layer(0, \"./model\", model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxBvPQ-sA7te"
      },
      "outputs": [],
      "source": [
        "layer = model.model.layers[0].self_attn.q_proj.to(DEVICE)\n",
        "unquantized = layer.weight.data.clone()\n",
        "inps = inps.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_bHg11of5_n"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azMTK8AwA8sl"
      },
      "outputs": [],
      "source": [
        "def get_accumulate_input_fn(name, hessians, num_samples):\n",
        "    def tmp(_, inp, out):\n",
        "        inp = inp[0].data # ... x hidden_size\n",
        "        inp = inp.reshape((-1, inp.shape[-1])) # inputs x hidden_size\n",
        "        inp = inp.t().float() # hidden_size x inputs\n",
        "        num_samples[name] += 1\n",
        "        if hessians[name] is None:\n",
        "            hessians[name] = inp.matmul(inp.t())\n",
        "        else:\n",
        "            hessians[name] += inp.matmul(inp.t())\n",
        "    return tmp\n",
        "\n",
        "hessians = {\"\": None}\n",
        "num_samples = {\"\": 0}\n",
        "fn = get_accumulate_input_fn(\"\", hessians, num_samples)\n",
        "for inp in inps:\n",
        "    fn(None, inp, None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95Q-Tu4OaLz2"
      },
      "outputs": [],
      "source": [
        "OUT = layer.weight.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Nc0uwwzDc-Y"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    baseline = torch.stack(tuple(layer(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxEh22JiA-lw",
        "outputId": "b8842c82-c708-4548-f2ea-ca70db23f07e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.0011, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    quantized_weight, scale, zero, outlier_weight, perm = quik(layer.weight.data, 4, 2 * hessians[\"\"] / num_samples[\"\"], n_outliers=256)\n",
        "    shit = QuikLinear(quantized_weight, scale, zero, outlier_weight, layer.bias, 8, perm)\n",
        "\n",
        "    start = time.time()\n",
        "    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n",
        "    # print(time.time() - start)\n",
        "\n",
        "    print(torch.pow(result - baseline, 2).mean() ** (1/2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLrSrDHjUVGo",
        "outputId": "ece8f98f-d918-41c4-ede6-3aa3b94dddc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.0012, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    quantized_weight, scale, zero = gptq(layer.weight.data, 4, 2 * hessians[\"\"] / num_samples[\"\"])\n",
        "    shit = QuantizedLinear(quantized_weight, scale, zero, layer.bias)\n",
        "\n",
        "    start = time.time()\n",
        "    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n",
        "    # print(time.time() - start)\n",
        "\n",
        "    print(torch.pow(result - baseline, 2).mean() ** (1/2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Lq-NrlPa8vo",
        "outputId": "ac1217c3-d373-48ee-959b-e69aa2f86b6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.0016, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    quantized_weight, scale, zero = custom_quantize(layer.weight.data, 4)\n",
        "    shit = QuantizedLinear(quantized_weight, scale, zero, layer.bias)\n",
        "\n",
        "    start = time.time()\n",
        "    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n",
        "    # print(time.time() - start)\n",
        "\n",
        "    print(torch.pow(result - baseline, 2).mean() ** (1/2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6ypQwVhbY9l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ipghRjSA1jVN",
        "agCFfP7x2Au6"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}